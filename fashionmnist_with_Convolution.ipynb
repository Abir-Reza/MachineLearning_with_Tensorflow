{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fashionmnist_with_Convolution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMeShxg+IG+pPh8x19WpjCX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abir-Reza/MachineLearning_with_Tensorflow/blob/master/fashionmnist_with_Convolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIhqwshU2imY"
      },
      "source": [
        "#Training Without Convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFjG956_2KbT",
        "outputId": "c2d10db9-f856-4295-e416-ce14e08370bc"
      },
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images=training_images / 255.0\n",
        "test_images=test_images / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6233 - accuracy: 0.7875\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3868 - accuracy: 0.8616\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3390 - accuracy: 0.8747\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3146 - accuracy: 0.8850\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2940 - accuracy: 0.8915\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.3474 - accuracy: 0.8736\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDRfJ8_TJu26"
      },
      "source": [
        "#**Result :**\n",
        "89% Accuracy on training data . <br>\n",
        "87% on test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TKuC6-l3GTa"
      },
      "source": [
        "#**Training with convolution**\n",
        "Input image is $28*28$ pixel.<br>\n",
        "\n",
        "###**Model structure:** <br>\n",
        "64, number of $3*3$ filter .<br>\n",
        "$2*2$ max pooling.<br>\n",
        "64, $3*3$ filter.<br>\n",
        "$2*2$ pax pooling.\n",
        "\n",
        "128 fully connected neural network.<br>\n",
        "10 classification in output fuction.\n",
        "\n",
        "##**Note :**\n",
        "Step 1 is to gather the data. You'll notice that there's a bit of a change here in that the training **data needed to be reshaped**. That's because the **first convolution expects** a **single tensor** containing everything, so instead of 60,000 28x28x1 items in a list, we have ***a single 4D list*** that is 60,000x28x28x1, and the same for the test images. If you don't do this, you'll get an error when training as the Convolutions do not recognize the shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GShVdceP5VcP",
        "outputId": "7723a6a8-8350-4a7b-86c1-36343f39ec33"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images,training_labels),(test_images,test_labels)= mnist.load_data()\n",
        "\n",
        "#reshape training image from 3 channel(color image) to 1 channel(grey scale)\n",
        "#there are 60000 image in training set, each 28*28 pizel\n",
        "training_images = training_images.reshape(60000, 28,28,1)\n",
        "training_images = training_images/255.0\n",
        "\n",
        "#reshape test image\n",
        "test_images = test_images.reshape(10000, 28,28,1)\n",
        "test_images = test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1)),\n",
        "  tf.keras.layers.MaxPool2D(2,2),\n",
        "  tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "  tf.keras.layers.MaxPool2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation ='softmax') \n",
        "])\n",
        "\n",
        "model.compile(optimizer ='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiHeLLxrDTVY",
        "outputId": "ab39d6eb-9430-42d9-d4ae-ce6d82bac1d6"
      },
      "source": [
        "#Training\n",
        "model.fit(training_images,training_labels,epochs= 5)\n",
        "\n",
        "#Test\n",
        "test_loss = model.evaluate(test_images,test_labels)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 76s 40ms/step - loss: 0.5962 - accuracy: 0.7814\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 75s 40ms/step - loss: 0.3005 - accuracy: 0.8896\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 75s 40ms/step - loss: 0.2413 - accuracy: 0.9108\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 75s 40ms/step - loss: 0.2124 - accuracy: 0.9214\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 75s 40ms/step - loss: 0.1840 - accuracy: 0.9309\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.2570 - accuracy: 0.9069\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zw637u8KVgj"
      },
      "source": [
        "#**Result using convolution :**\n",
        "93% accuracy on training data. <br>\n",
        "90% accuracy on test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVoPa0iJMW87"
      },
      "source": [
        "# Visualizing the Convolutions and Pooling\n",
        "\n",
        "This code will show us the convolutions graphically. The print (test_labels[;100]) shows us the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the DNN is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns_3CtBnMmZ8",
        "outputId": "3558218b-91e2-413b-d844-2f93a13e2091"
      },
      "source": [
        "print(test_labels[:100])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
            " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
            " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8NK-XOEeMl5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f,axarr = plt.subplot(3,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7UjRdcTlFnc"
      },
      "source": [
        "**EXERCISES**\n",
        "\n",
        "1. Try editing the convolutions. Change the **32s** to either** 16 or 64**. What impact will this have on accuracy and/or training time.\n",
        "\n",
        "2. **Remove the final Convoluti**on. What impact will this have on accuracy or training time?\n",
        "\n",
        "3. **How about adding more Convolutions**? What impact do you think this will have? Experiment with it.\n",
        "\n",
        "4. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it. \n",
        "\n",
        "5. In the previous lesson you **implemented a callback** to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsU5wTtQoD_z"
      },
      "source": [
        "1. **Using 32 filter.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "DJ67EO2NlMKY",
        "outputId": "c940789f-7791-4eb6-b9ea-6da57ef774ee"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images,training_labels),(test_images,test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images.reshape(60000,28,28,1)\n",
        "training_images = training_images/255.0\n",
        "\n",
        "test_images = test_images.reshape(10000,28,28,1)\n",
        "test_images = test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3),activation='relu', input_shape=(28,28,1)),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128,activation='relu'),\n",
        "    tf.keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "#model optimize\n",
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "#model train\n",
        "model.fit(training_images,training_labels,epochs=10)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images,test_labels)\n",
        "\n",
        "#print( 'accuracy is : ' +test_acc)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.5263 - accuracy: 0.8134\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.2668 - accuracy: 0.9039\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.2176 - accuracy: 0.9213\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.1841 - accuracy: 0.9320\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.1548 - accuracy: 0.9433\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.1361 - accuracy: 0.9491\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.1154 - accuracy: 0.9582\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0964 - accuracy: 0.9640\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0803 - accuracy: 0.9708\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.0685 - accuracy: 0.9748\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.3106 - accuracy: 0.9191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-69645c8f9aa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'accuracy is : '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpTc-4P6uzY6",
        "outputId": "1611f03a-9026-4d49-8a20-b5cfabb144b2"
      },
      "source": [
        "print(test_acc)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9190999865531921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aebTaxmIu-XL"
      },
      "source": [
        "#Verdict :\n",
        "\n",
        "97% accuracy in training data. <br>\n",
        "\n",
        "91% acccuracy in test data."
      ]
    }
  ]
}